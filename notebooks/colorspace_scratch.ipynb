{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colorspace Formalization\n",
    "- Idea: AEGAN uses a color module. This module assumes the output of a convolutional Generator will have 16 channels that are then softmaxed to pick a dominant color each for R, G, B\n",
    "- Implementation uses a lot of duplicated code, which could be wrapped in a loop BUT I have a better idea\n",
    "- This feels like it should be possible to do directly with matrix multiplication, but the batch dimension makes things slightly tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canof\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some variables:\n",
    "image_size = 5\n",
    "batch_size = 4\n",
    "color_channels = 2\n",
    "intermediate_channels = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# input upsampled \n",
    "im = softmax(torch.randn((intermediate_channels, image_size, image_size)))\n",
    "\n",
    "# I think we need to be careful when constructing the palette\n",
    "palette = torch.rand(intermediate_channels, color_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3240, 0.2982],\n",
       "         [0.3424, 0.4572],\n",
       "         [0.3093, 0.3901],\n",
       "         [1.0629, 1.2004],\n",
       "         [0.3918, 0.6281]],\n",
       "\n",
       "        [[0.2763, 0.3047],\n",
       "         [0.8138, 1.1029],\n",
       "         [0.7074, 0.8083],\n",
       "         [0.4189, 0.4584],\n",
       "         [0.2139, 0.2998]],\n",
       "\n",
       "        [[0.3711, 0.4637],\n",
       "         [0.3720, 0.5581],\n",
       "         [0.7408, 0.7364],\n",
       "         [0.5720, 0.7847],\n",
       "         [0.3744, 0.4311]],\n",
       "\n",
       "        [[0.4013, 0.4718],\n",
       "         [0.6303, 0.7126],\n",
       "         [0.3697, 0.4774],\n",
       "         [0.4200, 0.4611],\n",
       "         [0.6091, 0.8513]],\n",
       "\n",
       "        [[0.4939, 0.7284],\n",
       "         [0.4190, 0.6857],\n",
       "         [0.4041, 0.3374],\n",
       "         [0.3818, 0.3744],\n",
       "         [0.7315, 0.8481]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.transpose_(0, 2) @ palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial color size: torch.Size([4, 4])\n",
      "Resized: torch.Size([4, 4, 1, 1])\n",
      "Upsampled: torch.Size([4, 4, 5, 5])\n",
      "Post Mult Size: torch.Size([4, 4, 5, 5])\n",
      "Post Agg Size: torch.Size([4, 1, 5, 5])\n",
      "Initial color size: torch.Size([4, 4])\n",
      "Resized: torch.Size([4, 4, 1, 1])\n",
      "Upsampled: torch.Size([4, 4, 5, 5])\n",
      "Post Mult Size: torch.Size([4, 4, 5, 5])\n",
      "Post Agg Size: torch.Size([4, 1, 5, 5])\n",
      "Output Image: torch.Size([4, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# how it currently works\n",
    "im = softmax(torch.randn((batch_size, intermediate_channels, image_size, image_size)))\n",
    "color_palettes = [torch.rand((batch_size, intermediate_channels)) for _ in range(color_channels)]\n",
    "upsampler = nn.Upsample(scale_factor=image_size)\n",
    "\n",
    "outs = []\n",
    "\n",
    "for color in color_palettes:\n",
    "    print(f\"Initial color size: {color.shape}\")\n",
    "    # then a reshape occurs to match (b, c, im, im)\n",
    "    color = color.view((-1, intermediate_channels, 1, 1))\n",
    "    print(f\"Resized: {color.shape}\")\n",
    "    # upsamples the reshaped tensor\n",
    "    color = upsampler(color)\n",
    "    print(f\"Upsampled: {color.shape}\")\n",
    "\n",
    "    # next comes the multiplication of the two matrices (I assume elementwise)\n",
    "    inter_im = im * color\n",
    "    print(f\"Post Mult Size: {inter_im.shape}\")\n",
    "\n",
    "    # final aggregation step\n",
    "    out_im = torch.sum(inter_im, dim=1, keepdim=True)\n",
    "    print(f\"Post Agg Size: {out_im.shape}\")\n",
    "\n",
    "    outs.append(out_im)\n",
    "\n",
    "final_im = torch.cat(outs, dim=1)\n",
    "print(f\"Output Image: {final_im.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size: torch.Size([4, 5, 5])\t Palette Size: torch.Size([4, 2])\n",
      "Final Shape: torch.Size([2, 5, 5])\n",
      "Image Size: torch.Size([4, 5, 5])\t Palette Size: torch.Size([4, 2])\n",
      "Final Shape: torch.Size([2, 5, 5])\n",
      "Image Size: torch.Size([4, 5, 5])\t Palette Size: torch.Size([4, 2])\n",
      "Final Shape: torch.Size([2, 5, 5])\n",
      "Image Size: torch.Size([4, 5, 5])\t Palette Size: torch.Size([4, 2])\n",
      "Final Shape: torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 5, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempt to recreate this\n",
    "\n",
    "palettes = torch.rand((batch_size, intermediate_channels, color_channels))\n",
    "outs = []\n",
    "\n",
    "for curr_im, color in zip(im, palettes):\n",
    "    print(f\"Image Size: {curr_im.shape}\\t Palette Size: {color.shape}\")\n",
    "    out_im = curr_im.transpose_(0, 2) @ color\n",
    "    out_im.transpose_(0, 2)\n",
    "    print(f\"Final Shape: {out_im.shape}\")\n",
    "    outs.append(out_im.view(-1, color_channels, image_size, image_size))\n",
    "\n",
    "out_im = torch.cat(outs, dim=0)\n",
    "out_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make them classes and compare!\n",
    "\n",
    "class OldMethod(nn.Module):\n",
    "    def __init__(self, colors, im_size, nc):\n",
    "        super(OldMethod, self).__init__()\n",
    "        self.colors = colors\n",
    "        self.upsampler = nn.Upsample(scale_factor=im_size)\n",
    "        self.nc = nc\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = []\n",
    "        for color in self.colors:\n",
    "            color = color.view((-1, self.nc, 1, 1))\n",
    "            color = self.upsampler(color)\n",
    "            out_im = input * color\n",
    "            out_im = torch.sum(out_im, dim=1, keepdim=True)\n",
    "            output.append(out_im)\n",
    "        \n",
    "        return torch.cat(output, dim=1)\n",
    "\n",
    "class NewMethod(nn.Module):\n",
    "    def __init__(self, colors, nc, outc):\n",
    "        super(NewMethod, self).__init__()\n",
    "        self.colors = colors.view(-1, 1, nc, outc) #(b, nc, outc) -> (b, 1, nc, outc)\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits = input.transpose_(1, 3)\n",
    "        logits = logits @ self.colors\n",
    "        logits = logits.transpose_(1, 3)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 8           # batch size\n",
    "im_size = 5     # image size\n",
    "nc = 3          # intermediate palette channels\n",
    "outc = 2        # total colors to output\n",
    "\n",
    "color_list = [torch.rand(b, nc) for _ in range(outc)] # a list of (b, nc) tensors, length outc\n",
    "reshuffled_colors = [color.view(-1, nc, 1) for color in color_list] \n",
    "color_tensor = torch.cat(reshuffled_colors, dim=2) # creates a (b, nc, outc) tensor\n",
    "\n",
    "im = softmax(torch.rand(b, nc, im_size, im_size))\n",
    "\n",
    "champion = OldMethod(colors=color_list, im_size=im_size, nc=nc)\n",
    "challenger = NewMethod(color_tensor, nc, outc)\n",
    "\n",
    "old_out = champion(im)\n",
    "new_out = challenger(im)\n",
    "\n",
    "torch.allclose(old_out, new_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREAT SUCCESS !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
